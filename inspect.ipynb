{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:27:12.325802Z","iopub.status.busy":"2023-04-27T07:27:12.325379Z","iopub.status.idle":"2023-04-27T07:28:02.010025Z","shell.execute_reply":"2023-04-27T07:28:02.008785Z","shell.execute_reply.started":"2023-04-27T07:27:12.325766Z"},"trusted":true},"outputs":[],"source":["# #Installing Relevant Packages\n","# !pip install gdown \n","# !pip install mirdata --user\n","# !pip install madmom --user\n","\n","# !pip install torchmetrics --user\n","# !pip install spleeter --user"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:02.014541Z","iopub.status.busy":"2023-04-27T07:28:02.014194Z","iopub.status.idle":"2023-04-27T07:28:02.021691Z","shell.execute_reply":"2023-04-27T07:28:02.020552Z","shell.execute_reply.started":"2023-04-27T07:28:02.014505Z"},"trusted":true},"outputs":[],"source":["import os\n","import mirdata\n","import torch\n","import torch.nn as nn\n","# from matplotlib import pyplot as plt\n","import random\n","import numpy as np\n","import librosa\n","import torchaudio\n","import madmom\n","from tqdm import tqdm as tqdm\n","import torchmetrics\n","import warnings\n","from torch.nn import TransformerEncoderLayer as torchTransformerEncoderLayer\n","import pickle"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:02.024481Z","iopub.status.busy":"2023-04-27T07:28:02.023321Z","iopub.status.idle":"2023-04-27T07:28:02.035853Z","shell.execute_reply":"2023-04-27T07:28:02.034753Z","shell.execute_reply.started":"2023-04-27T07:28:02.024442Z"},"trusted":true},"outputs":[],"source":["# ignore certain warnings\n","warnings.filterwarnings('ignore')\n","\n","# set default figure size\n","# plt.rcParams['figure.figsize'] = (20, 6)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:02.039891Z","iopub.status.busy":"2023-04-27T07:28:02.039548Z","iopub.status.idle":"2023-04-27T07:28:02.049846Z","shell.execute_reply":"2023-04-27T07:28:02.048689Z","shell.execute_reply.started":"2023-04-27T07:28:02.039856Z"},"trusted":true},"outputs":[],"source":["#Ensure deterministic behaviour\n","torch.backends.cudnn.deterministic = True\n","random.seed(hash(\"Setting random hash\") % 2**32 - 1)\n","torch.manual_seed(hash(\"By removing stochasticity\") % 2**32 - 1)\n","torch.cuda.manual_seed_all(hash(\"So runs are repeatable\") % 2**32 - 1)\n","np.random.seed(hash(\"Improves reducibility\") % 2**32 - 1)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:02.053266Z","iopub.status.busy":"2023-04-27T07:28:02.052996Z","iopub.status.idle":"2023-04-27T07:28:02.059766Z","shell.execute_reply":"2023-04-27T07:28:02.058752Z","shell.execute_reply.started":"2023-04-27T07:28:02.053240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda:7\n"]}],"source":["working_dir = '/raid/home/niranjan20090/DL/'\n","device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:',device)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:02.062102Z","iopub.status.busy":"2023-04-27T07:28:02.061359Z","iopub.status.idle":"2023-04-27T07:28:02.067141Z","shell.execute_reply":"2023-04-27T07:28:02.066071Z","shell.execute_reply.started":"2023-04-27T07:28:02.062065Z"},"trusted":true},"outputs":[],"source":["sample_rate = 16000\n","\n","config = dict(\n","    sample_rate = sample_rate,\n","    FPS = 15.625,    \n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:02.069597Z","iopub.status.busy":"2023-04-27T07:28:02.068861Z","iopub.status.idle":"2023-04-27T07:28:04.039584Z","shell.execute_reply":"2023-04-27T07:28:04.038388Z","shell.execute_reply.started":"2023-04-27T07:28:02.069557Z"},"trusted":true},"outputs":[],"source":["# !mkdir dataset\n","# !mkdir models"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:04.043204Z","iopub.status.busy":"2023-04-27T07:28:04.042865Z","iopub.status.idle":"2023-04-27T07:28:05.698731Z","shell.execute_reply":"2023-04-27T07:28:05.697486Z","shell.execute_reply.started":"2023-04-27T07:28:04.043167Z"},"trusted":true},"outputs":[],"source":["# # #Small Dataset\n","# gtzan = mirdata.initialize('gtzan_genre', version='mini',data_home=os.path.join(working_dir,'dataset'))\n","# gtzan.download()\n","# print(len(gtzan.track_ids))\n","\n","# track_ids = gtzan.track_ids\n","# print(gtzan.track(track_ids[42]))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:05.700892Z","iopub.status.busy":"2023-04-27T07:28:05.700487Z","iopub.status.idle":"2023-04-27T07:28:13.513075Z","shell.execute_reply":"2023-04-27T07:28:13.511991Z","shell.execute_reply.started":"2023-04-27T07:28:05.700852Z"},"trusted":true},"outputs":[],"source":["# # https://drive.google.com/file/d//view?usp=share_link\n","# import gdown\n","# idx = \"1nP9azGmjUCcigCN-wipIDFZuohzkolXL\"\n","# out = \"dataset/gtzan_mini.tar.gz\"\n","# gdown.download(id=idx,output=out,quiet=False)\n","# !tar -xf dataset/gtzan_mini.tar.gz\n","# !mv gtzan_mini dataset/"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.515269Z","iopub.status.busy":"2023-04-27T07:28:13.514864Z","iopub.status.idle":"2023-04-27T07:28:13.523590Z","shell.execute_reply":"2023-04-27T07:28:13.522203Z","shell.execute_reply.started":"2023-04-27T07:28:13.515227Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Downloading ['all', 'tempo_beat_annotations'] to /raid/home/niranjan20090/mir_datasets/gtzan_genre\n","INFO: [all] downloading genres.tar.gz\n","INFO: /raid/home/niranjan20090/mir_datasets/gtzan_genre/gtzan_genre/genres.tar.gz already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n","INFO: [tempo_beat_annotations] downloading annot.zip\n","INFO: /raid/home/niranjan20090/mir_datasets/gtzan_genre/annot.zip already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n"]},{"name":"stdout","output_type":"stream","text":["1000\n","Track(\n","  audio_path=\"/raid/home/niranjan20090/mir_datasets/gtzan_genre/gtzan_genre/genres/classical/classical.00042.wav\",\n","  beats_path=\".../home/niranjan20090/mir_datasets/gtzan_genre/gtzan_tempo_beat-main/beats/gtzan_classical_00042.beats\",\n","  genre=\"classical\",\n","  tempo_path=\"...id/home/niranjan20090/mir_datasets/gtzan_genre/gtzan_tempo_beat-main/tempo/gtzan_classical_00042.bpm\",\n","  track_id=\"classical.00042\",\n","  audio: The track's audio\n","\n","        Returns,\n","  beats: ,\n","  tempo: ,\n",")\n"]}],"source":["#Large dataset\n","# use the following line to initialise the dataset (i.e. the full version without 'mini')\n","gtzan = mirdata.initialize('gtzan_genre')\n","gtzan.download()\n","print(len(gtzan.track_ids))\n","\n","track_ids = gtzan.track_ids\n","print(gtzan.track(track_ids[42]))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.527271Z","iopub.status.busy":"2023-04-27T07:28:13.525729Z","iopub.status.idle":"2023-04-27T07:28:13.540272Z","shell.execute_reply":"2023-04-27T07:28:13.539256Z","shell.execute_reply.started":"2023-04-27T07:28:13.527233Z"},"trusted":true},"outputs":[],"source":["# # https://drive.google.com/file/d/18_RfquqT15dDRPoglV8ZhV5qIbAliOpG/view?usp=share_link\n","# import gdown\n","# idx = \"18_RfquqT15dDRPoglV8ZhV5qIbAliOpG\"\n","# out = \"dataset/gtzan_full.zip\"\n","# gdown.download(id=idx,output=out,quiet=False)\n","\n","# !unzip dataset/gtzan_full.zip"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.542484Z","iopub.status.busy":"2023-04-27T07:28:13.541953Z","iopub.status.idle":"2023-04-27T07:28:13.555005Z","shell.execute_reply":"2023-04-27T07:28:13.553673Z","shell.execute_reply.started":"2023-04-27T07:28:13.542447Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(700, 150, 150)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import train_test_split\n","\n","tracks = gtzan.load_tracks()\n","train_files, test_files = train_test_split(list(tracks.keys()), test_size=0.3, random_state=1234)\n","val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=1234)\n","len(train_files), len(val_files), len(test_files)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.557385Z","iopub.status.busy":"2023-04-27T07:28:13.556897Z","iopub.status.idle":"2023-04-27T07:28:13.570891Z","shell.execute_reply":"2023-04-27T07:28:13.569873Z","shell.execute_reply.started":"2023-04-27T07:28:13.557346Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","class GtzanDataset(Dataset):\n","    def __init__(self,track_ids,device,sample_rate,fps,data_path):\n","        self.track_ids = []\n","        self.device = device\n","        self.sample_rate = sample_rate\n","        self.fps = fps\n","        self.duration = 7\n","        self.data_path = data_path\n","        for track_id in track_ids:\n","            track = gtzan.track(track_id)\n","            try:\n","                beats = track.beats.times\n","                downbeats = track.beats.positions.astype(int) == 1\n","            except AttributeError:\n","                continue\n","            self.track_ids.append(track_id)\n","\n","                \n","    def __len__(self):\n","        return len(self.track_ids)\n","    \n","    def __getitem__(self,idx):\n","        track_id = self.track_ids[idx]\n","        track = gtzan.track(track_id)\n","        audios = np.load(os.path.join(data_path,f'{track_id}.npy'))\n","        \n","        n_frames = int(self.fps*self.duration)*4\n","        beats = track.beats.times\n","        downbeats = track.beats.positions.astype(int) == 1\n","        downbeats = track.beats.times[downbeats]\n","        \n","        beats = track.beats.times[track.beats.positions.astype(int) != 1] #CE\n","        \n","        beats = madmom.utils.quantize_events(beats,fps=self.fps,length=n_frames)\n","        downbeats = madmom.utils.quantize_events(downbeats,fps=self.fps,length=n_frames)\n","        nonbeats = (beats == 0)&(downbeats == 0)\n","                \n","        beats = beats.reshape(beats.shape[0],1)\n","        downbeats = downbeats.reshape(downbeats.shape[0],1)\n","        nonbeats = nonbeats.reshape(nonbeats.shape[0],1)\n","        \n","        truths = np.hstack([beats,downbeats,nonbeats])\n","        \n","#         feats = torch.from_numpy(feats).to(device).type(torch.FloatTensor)\n","        truths = torch.from_numpy(truths).to(device).type(torch.FloatTensor)       \n","\n","        return {\n","            'audio': [audios[:,i*self.duration*self.sample_rate:(i+1)*self.duration*self.sample_rate] for i in range(4)],\n","            'targets': truths,\n","            'id': track_id,\n","        }"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.572854Z","iopub.status.busy":"2023-04-27T07:28:13.572396Z","iopub.status.idle":"2023-04-27T07:28:13.641175Z","shell.execute_reply":"2023-04-27T07:28:13.640182Z","shell.execute_reply.started":"2023-04-27T07:28:13.572813Z"},"trusted":true},"outputs":[],"source":["# data_path = '/raid/home/niranjan20090/DL/dataset/gtzan_mini'\n","data_path = '/raid/home/niranjan20090/DL/dataset/gtzan_full'\n","train_dataset = GtzanDataset(track_ids=train_files,device=device,\n","                             sample_rate=config['sample_rate'],fps=config['FPS'],data_path=data_path)\n","val_dataset = GtzanDataset(track_ids=val_files,device=device,\n","                           sample_rate=config['sample_rate'],fps=config['FPS'],data_path=data_path)\n","test_dataset = GtzanDataset(track_ids=test_files,device=device,\n","                            sample_rate=config['sample_rate'],fps=config['FPS'],data_path=data_path)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.642763Z","iopub.status.busy":"2023-04-27T07:28:13.642387Z","iopub.status.idle":"2023-04-27T07:28:13.648965Z","shell.execute_reply":"2023-04-27T07:28:13.647793Z","shell.execute_reply.started":"2023-04-27T07:28:13.642697Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","config['batch_size'] = 2\n","\n","train_loader = DataLoader(dataset=train_dataset,batch_size=config['batch_size'],shuffle=True,drop_last=True)\n","val_loader = DataLoader(dataset=val_dataset,batch_size=config['batch_size'],shuffle=False,drop_last=True)\n","test_loader = DataLoader(dataset=test_dataset,batch_size=config['batch_size'],shuffle=False,drop_last=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.651372Z","iopub.status.busy":"2023-04-27T07:28:13.650567Z","iopub.status.idle":"2023-04-27T07:28:13.673947Z","shell.execute_reply":"2023-04-27T07:28:13.672948Z","shell.execute_reply.started":"2023-04-27T07:28:13.651323Z"},"trusted":true},"outputs":[],"source":["#Copied from https://github.com/MWM-io/SpecTNT-pytorch/blob/master/harmonicstft.py\n","def hz_to_midi(hz):\n","    return 12 * (torch.log2(hz) - np.log2(440.0)) + 69\n","\n","\n","def midi_to_hz(midi):\n","    return 440.0 * (2.0 ** ((midi - 69.0)/12.0))\n","\n","\n","def note_to_midi(note):\n","    return librosa.core.note_to_midi(note)\n","\n","\n","def hz_to_note(hz):\n","    return librosa.core.hz_to_note(hz)\n","\n","\n","def initialize_filterbank(sample_rate, n_harmonic, semitone_scale):\n","    # MIDI\n","    # lowest note\n","    low_midi = note_to_midi('C1')\n","    # highest note\n","    high_note = hz_to_note(sample_rate / (2 * n_harmonic))\n","    high_midi = note_to_midi(high_note)\n","    # number of scales\n","    level = (high_midi - low_midi) * semitone_scale\n","    midi = np.linspace(low_midi, high_midi, level + 1)\n","    hz = midi_to_hz(midi[:-1])\n","    # stack harmonics\n","    harmonic_hz = []\n","    for i in range(n_harmonic):\n","        harmonic_hz = np.concatenate((harmonic_hz, hz * (i+1)))\n","    return harmonic_hz, level\n","\n","\n","class HarmonicSTFT(nn.Module):\n","    \"\"\"\n","    Trainable harmonic filters as implemented by Minz Won.\n","    \n","    Paper: https://ccrma.stanford.edu/~urinieto/MARL/publications/ICASSP2020_Won.pdf\n","    Code: https://github.com/minzwon/data-driven-harmonic-filters\n","    Pretrained: https://github.com/minzwon/sota-music-tagging-models/tree/master/training\n","    \"\"\"\n","\n","    def __init__(self,\n","                 sample_rate=16000,\n","                 n_fft=512,\n","                 win_length=None,\n","                 hop_length=None,\n","                 pad=0,\n","                 power=2,\n","                 normalized=False,\n","                 n_harmonic=6,\n","                 semitone_scale=2,\n","                 bw_Q=1.0,\n","                 learn_bw='only_Q',\n","                 checkpoint=None):\n","        super(HarmonicSTFT, self).__init__()\n","\n","        # Parameters\n","        self.sample_rate = sample_rate\n","        self.n_harmonic = n_harmonic\n","        self.bw_alpha = 0.1079\n","        self.bw_beta = 24.7\n","\n","        # Spectrogram\n","        self.spec = torchaudio.transforms.Spectrogram(n_fft=n_fft, win_length=win_length,\n","                                                      hop_length=hop_length, pad=pad,\n","                                                      window_fn=torch.hann_window,\n","                                                      power=power, normalized=normalized, wkwargs=None)\n","        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n","\n","        # Initialize the filterbank. Equally spaced in MIDI scale.\n","        harmonic_hz, self.level = initialize_filterbank(\n","            sample_rate, n_harmonic, semitone_scale)\n","\n","        # Center frequncies to tensor\n","        self.f0 = torch.tensor(harmonic_hz.astype('float32'))\n","\n","        # Bandwidth parameters\n","        if learn_bw == 'only_Q':\n","            self.bw_Q = nn.Parameter(torch.tensor(\n","                np.array([bw_Q]).astype('float32')))\n","        elif learn_bw == 'fix':\n","            self.bw_Q = torch.tensor(np.array([bw_Q]).astype('float32'))\n","\n","        if checkpoint is not None:\n","            state_dict = torch.load(checkpoint)\n","            hstft_state_dict = {k.replace('hstft.', ''): v for k,\n","                                v in state_dict.items() if 'hstft.' in k}\n","            self.load_state_dict(hstft_state_dict)\n","\n","    def get_harmonic_fb(self):\n","        # bandwidth\n","        bw = (self.bw_alpha * self.f0 + self.bw_beta) / self.bw_Q\n","        bw = bw.unsqueeze(0)  # (1, n_band)\n","        f0 = self.f0.unsqueeze(0)  # (1, n_band)\n","        fft_bins = self.fft_bins.unsqueeze(1)  # (n_bins, 1)\n","\n","        up_slope = torch.matmul(fft_bins, (2/bw)) + 1 - (2 * f0 / bw)\n","        down_slope = torch.matmul(fft_bins, (-2/bw)) + 1 + (2 * f0 / bw)\n","        fb = torch.max(self.zero, torch.min(down_slope, up_slope))\n","        return fb\n","\n","    def to_device(self, device, n_bins):\n","        self.f0 = self.f0.to(device)\n","        self.bw_Q = self.bw_Q.to(device)\n","        # fft bins\n","        self.fft_bins = torch.linspace(0, self.sample_rate//2, n_bins)\n","        self.fft_bins = self.fft_bins.to(device)\n","        self.zero = torch.zeros(1)\n","        self.zero = self.zero.to(device)\n","\n","    def forward(self, waveform):\n","        # stft\n","        spectrogram = self.spec(waveform)\n","        # to device\n","        self.to_device(waveform.device, spectrogram.size(1))\n","        # triangle filter\n","        harmonic_fb = self.get_harmonic_fb()\n","        harmonic_spec = torch.matmul(\n","            spectrogram.transpose(1, 2), harmonic_fb).transpose(1, 2)\n","        # (batch, channel, length) -> (batch, harmonic, f0, length)\n","        b, c, l = harmonic_spec.size()\n","        harmonic_spec = harmonic_spec.view(b, self.n_harmonic, self.level, l)\n","        # amplitude to db\n","        harmonic_spec = self.amplitude_to_db(harmonic_spec)\n","        return harmonic_spec"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.677608Z","iopub.status.busy":"2023-04-27T07:28:13.677231Z","iopub.status.idle":"2023-04-27T07:28:13.702021Z","shell.execute_reply":"2023-04-27T07:28:13.700915Z","shell.execute_reply.started":"2023-04-27T07:28:13.677553Z"},"trusted":true},"outputs":[],"source":["#Resnet Front-End\n","#Taken from https://github.com/MWM-io/SpecTNT-pytorch/blob/master/networks.py\n","\n","class Res2DMaxPoolModule(nn.Module):\n","    def __init__(self, in_channels, out_channels, pooling=2):\n","        super(Res2DMaxPoolModule, self).__init__()\n","        self.conv_1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n","        self.bn_1 = nn.BatchNorm2d(out_channels)\n","        self.conv_2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n","        self.bn_2 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","        self.mp = nn.MaxPool2d(tuple(pooling))\n","\n","        # residual\n","        self.diff = False\n","        if in_channels != out_channels:\n","            self.conv_3 = nn.Conv2d(\n","                in_channels, out_channels, 3, padding=1)\n","            self.bn_3 = nn.BatchNorm2d(out_channels)\n","            self.diff = True\n","\n","    def forward(self, x):\n","        out = self.bn_2(self.conv_2(self.relu(self.bn_1(self.conv_1(x)))))\n","        if self.diff:\n","            x = self.bn_3(self.conv_3(x))\n","        out = x + out\n","        out = self.mp(self.relu(out))\n","        return out\n","\n","\n","class ResFrontEnd(nn.Module):\n","    \"\"\"\n","    Adapted from Minz Won ResNet implementation.\n","    \n","    Original code: https://github.com/minzwon/semi-supervised-music-tagging-transformer/blob/master/src/modules.py\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, freq_pooling, time_pooling):\n","        super(ResFrontEnd, self).__init__()\n","        self.input_bn = nn.BatchNorm2d(in_channels)\n","        self.layer1 = Res2DMaxPoolModule(\n","            in_channels, out_channels, pooling=(freq_pooling[0], time_pooling[0]))\n","        self.layer2 = Res2DMaxPoolModule(\n","            out_channels, out_channels, pooling=(freq_pooling[1], time_pooling[1]))\n","        self.layer3 = Res2DMaxPoolModule(\n","            out_channels, out_channels, pooling=(freq_pooling[2], time_pooling[2]))\n","\n","    def forward(self, hcqt):\n","        \"\"\"\n","        Inputs:\n","            hcqt: [B, F, K, T]\n","        Outputs:\n","            out: [B, ^F, ^K, ^T]\n","        \"\"\"\n","        # batch normalization\n","        out = self.input_bn(hcqt)\n","        # CNN\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        return out\n","    \n","\n","class SpecTNTBlock(nn.Module):\n","    def __init__(\n","        self, n_channels=128, n_frequencies=16, n_times=109,\n","        spectral_dmodel=64, spectral_nheads=4, spectral_dimff=64,\n","        temporal_dmodel=256, temporal_nheads=8, temporal_dimff=256,\n","        embed_dim=128, dropout=0.15, use_tct=False\n","    ):\n","        super().__init__()\n","\n","        self.D = embed_dim\n","        self.F = n_frequencies\n","        self.K = n_channels\n","        self.T = n_times\n","\n","        # TCT: Temporal Class Token\n","        if use_tct:\n","            self.T += 1\n","\n","        # Shared frequency-time linear layers\n","        self.D_to_K = nn.Linear(self.D, self.K)\n","        self.K_to_D = nn.Linear(self.K, self.D)\n","\n","        # Spectral Transformer Encoder\n","        self.spectral_linear_in = nn.Linear(self.F+1, spectral_dmodel)\n","        self.spectral_encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=spectral_dmodel, nhead=spectral_nheads, dim_feedforward=spectral_dimff, dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n","        self.spectral_linear_out = nn.Linear(spectral_dmodel, self.F+1)\n","\n","        # Temporal Transformer Encoder\n","        self.temporal_linear_in = nn.Linear(self.T, temporal_dmodel)\n","        self.temporal_encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=temporal_dmodel, nhead=temporal_nheads, dim_feedforward=temporal_dimff, dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n","        self.temporal_linear_out = nn.Linear(temporal_dmodel, self.T)\n","\n","    def forward(self, spec_in, temp_in):\n","        \"\"\"\n","        Inputs:\n","            spec_in: spectral embedding input [B, T, F+1, K]\n","            temp_in: temporal embedding input [B, T, 1, D]\n","        Outputs:\n","            spec_out: spectral embedding output [B, T, F+1, K]\n","            temp_out: temporal embedding output [B, T, 1, D]\n","        \"\"\"\n","        # Element-wise addition between TE and FCT\n","        spec_in = spec_in + \\\n","            nn.functional.pad(self.D_to_K(temp_in), (0, 0, 0, self.F))\n","\n","        # Spectral Transformer\n","        spec_in = spec_in.flatten(0, 1).transpose(1, 2)  # [B*T, K, F+1]\n","        emb = self.spectral_linear_in(spec_in)  # [B*T, K, spectral_dmodel]\n","        spec_enc_out = self.spectral_encoder_layer(\n","            emb)  # [B*T, K, spectral_dmodel]\n","        spec_out = self.spectral_linear_out(spec_enc_out)  # [B*T, K, F+1]\n","        spec_out = spec_out.view(-1, self.T, self.K,\n","                                 self.F+1).transpose(2, 3)  # [B, T, F+1, K]\n","\n","        # FCT slicing (first raw) + back to D\n","        temp_in = temp_in + self.K_to_D(spec_out[:, :, :1, :])  # [B, T, 1, D]\n","\n","        # Temporal Transformer\n","        temp_in = temp_in.permute(0, 2, 3, 1).flatten(0, 1)  # [B, D, T]\n","        emb = self.temporal_linear_in(temp_in)  # [B, D, temporal_dmodel]\n","        temp_enc_out = self.temporal_encoder_layer(\n","            emb)  # [B, D, temporal_dmodel]\n","        temp_out = self.temporal_linear_out(temp_enc_out)  # [B, D, T]\n","        temp_out = temp_out.unsqueeze(1).permute(0, 3, 1, 2)  # [B, T, 1, D]\n","\n","        return spec_out, temp_out"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.704034Z","iopub.status.busy":"2023-04-27T07:28:13.703661Z","iopub.status.idle":"2023-04-27T07:28:13.718462Z","shell.execute_reply":"2023-04-27T07:28:13.716940Z","shell.execute_reply.started":"2023-04-27T07:28:13.703999Z"},"trusted":true},"outputs":[],"source":["#InSpecT module inspired from SpecTNT Module from https://github.com/MWM-io/SpecTNT-pytorch/blob/master/networks.py\n","\n","class SpecTNTModule(nn.Module):\n","    def __init__(\n","        self, n_channels=128, n_frequencies=16, n_times=109,\n","        spectral_dmodel=64, spectral_nheads=4, spectral_dimff=64,\n","        temporal_dmodel=256, temporal_nheads=8, temporal_dimff=256,\n","        embed_dim=128, dropout=0.15, use_tct=False, n_block=1\n","    ):\n","        super().__init__()\n","\n","        D = embed_dim\n","        F = n_frequencies\n","        K = n_channels\n","        T = n_times\n","\n","        # Frequency Class Token\n","        self.fct = nn.Parameter(torch.zeros(1, T, 1, K))\n","\n","        # Frequency Positional Encoding\n","        self.fpe = nn.Parameter(torch.zeros(1, 1, F+1, K))\n","\n","        # TCT: Temporal Class Token\n","        if use_tct:\n","            self.tct = nn.Parameter(torch.zeros(1, 1, 1, D))\n","        else:\n","            self.tct = None\n","\n","        # Temporal Embedding\n","        self.te = nn.Parameter(torch.rand(1, T, 1, D))\n","\n","        # SpecTNT blocks\n","        self.spectnt_blocks = nn.ModuleList([\n","            SpecTNTBlock(\n","                n_channels,\n","                n_frequencies,\n","                n_times,\n","                spectral_dmodel,\n","                spectral_nheads,\n","                spectral_dimff,\n","                temporal_dmodel,\n","                temporal_nheads,\n","                temporal_dimff,\n","                embed_dim,\n","                dropout,\n","                use_tct\n","            )\n","            for _ in range(n_block)\n","        ])\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Input:\n","            x: [B, T, F, K]\n","        Output:\n","            spec_emb: [B, T, F+1, K]\n","            temp_emb: [B, T, 1, D]\n","        \"\"\"\n","        batch_size = len(x)\n","\n","        # Initialize spectral embedding - concat FCT (first raw) + add FPE\n","        fct = torch.repeat_interleave(self.fct, batch_size, 0)  # [B, T, 1, K]\n","        spec_emb = torch.cat([fct, x], dim=2)  # [B, T, F+1, K]\n","        spec_emb = spec_emb + self.fpe\n","        if self.tct is not None:\n","            spec_emb = nn.functional.pad(\n","                spec_emb, (0, 0, 0, 0, 1, 0))  # [B, T+1, F+1, K]\n","\n","        # Initialize temporal embedding\n","        temp_emb = torch.repeat_interleave(self.te, batch_size, 0)  # [B, T, 1, D]\n","        if self.tct is not None:\n","            tct = torch.repeat_interleave(self.tct, batch_size, 0)  # [B, 1, 1, D]\n","            temp_emb = torch.cat([tct, temp_emb], dim=1)  # [B, T+1, 1, D]\n","\n","        # SpecTNT blocks inference\n","        for block in self.spectnt_blocks:\n","            spec_emb, temp_emb = block(spec_emb, temp_emb)\n","            \n","        return spec_emb, temp_emb"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.720972Z","iopub.status.busy":"2023-04-27T07:28:13.720343Z","iopub.status.idle":"2023-04-27T07:28:13.734927Z","shell.execute_reply":"2023-04-27T07:28:13.733773Z","shell.execute_reply.started":"2023-04-27T07:28:13.720942Z"},"trusted":true},"outputs":[],"source":["class InSpecTBlock(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.spectnt_block = SpecTNTBlock()\n","        self.inst_attn = torchTransformerEncoderLayer(d_model=128, nhead=8, \\\n","            dim_feedforward=1024, dropout=0.15, batch_first=True, activation=\"gelu\", norm_first=True)\n","        self.n_inst = 3\n","    \n","    def forward(self,spec_embs, temp_embs):\n","        '''\n","        Input\n","            spec_embs = [B T F+1 ES I]\n","            temp_embs = [B T 1 ET I]\n","        Output\n","            spec_embs = [B T F+1 ES I]\n","            temp_embs = [B T 1 ET I]\n","        '''\n","        B,T,F,ES,ET = spec_embs.shape[0], spec_embs.shape[1], \\\n","            spec_embs.shape[2]-1, spec_embs.shape[3], temp_embs.shape[3]\n","        t_spec_embs = []\n","        t_temp_embs = []\n","        for i in range(self.n_inst):\n","            spec_emb, temp_emb = self.spectnt_block(spec_embs[...,i],temp_embs[...,i])\n","            t_spec_embs.append(spec_emb)\n","            t_temp_embs.append(temp_emb)\n","        \n","        spec_embs = torch.stack(t_spec_embs,dim=4)\n","        temp_embs = torch.stack(t_temp_embs,dim=4)\n","        \n","        spec_embs = spec_embs.permute(0,1,2,4,3).reshape(-1,self.n_inst,ES) # [B*T*(F+1) I E]\n","        spec_embs = self.inst_attn(spec_embs)\n","        spec_embs = spec_embs.reshape(B,T,F+1,self.n_inst,ES).permute(0,1,2,4,3)\n","        \n","        temp_embs = temp_embs.permute(0,1,2,4,3).reshape(-1,self.n_inst,ET) # [B*T*1 I E]\n","        temp_embs = self.inst_attn(temp_embs)\n","        temp_embs = temp_embs.reshape(B,T,1,self.n_inst,ET).permute(0,1,2,4,3)\n","        return spec_embs, temp_embs"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.737294Z","iopub.status.busy":"2023-04-27T07:28:13.736437Z","iopub.status.idle":"2023-04-27T07:28:13.752430Z","shell.execute_reply":"2023-04-27T07:28:13.751375Z","shell.execute_reply.started":"2023-04-27T07:28:13.737257Z"},"trusted":true},"outputs":[],"source":["class InSpecT(nn.Module):\n","    def __init__(self, config, num_inspect_blocks=3):\n","        super().__init__()\n","        self.feature_extractor = HarmonicSTFT(config['sample_rate'])\n","        self.fe_model = ResFrontEnd(6,128,[2,2,2],[2,2,1])\n","        self.spectnt1 = SpecTNTModule(n_block=2)\n","        self.inspect_module = nn.ModuleList([InSpecTBlock(config) for i in range(num_inspect_blocks)])\n","        self.spectnt2 = SpecTNTModule(n_block=2)\n","        self.fc = nn.Linear(128,3)\n","        self.n_inst = 3\n","    \n","    def forward(self,audio):\n","        spec_embs = []\n","        temp_embs = []\n","        for i in range(self.n_inst):\n","            features = self.feature_extractor(audio[:,:,i])\n","            if len(features.size()) == 3:\n","                features = features.unsqueeze(1)\n","            fe_out = self.fe_model(features)\n","            fe_out = fe_out.permute(0,3,2,1)\n","            spec_emb, temp_emb = self.spectnt1(fe_out)\n","            spec_embs.append(spec_emb)\n","            temp_embs.append(temp_emb)\n","            \n","        spec_embs = torch.stack(spec_embs,dim=4)\n","        temp_embs = torch.stack(temp_embs,dim=4)\n","        for inspect_block in self.inspect_module:\n","            spec_embs, temp_embs = inspect_block(spec_embs,temp_embs)\n","        \n","        t_spec_embs = []\n","        t_temp_embs = []\n","        for i in range(self.n_inst):\n","            for block in self.spectnt2.spectnt_blocks:\n","                spec_emb, temp_emb = block(spec_embs[...,i],temp_embs[...,i])\n","            t_spec_embs.append(spec_emb)\n","            t_temp_embs.append(temp_emb)\n","        \n","        spec_embs = torch.stack(t_spec_embs,dim=4)\n","        temp_embs = torch.stack(t_temp_embs,dim=4)\n","        \n","        spec_emb = None\n","        temp_emb = None\n","        \n","        temp_embs = torch.relu(temp_embs)\n","\n","        temp_embs = torch.mean(temp_embs,dim=4).squeeze()\n","\n","        acts = self.fc(temp_embs)\n","        return acts\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["class ResBlock(nn.Module):\n","    def __init__(self,i,num_filters,kernel_size,padding,dropout_rate=0):\n","        super().__init__()\n","        self.i = i\n","        self.num_filters = num_filters\n","        self.kernel_size = kernel_size\n","        self.padding = padding\n","        self.dropout_rate = dropout_rate\n","        self.res_x = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=1, padding='same', bias=False)\n","        self.conv_3 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=1, padding='same', bias=False)\n","        self.conv_1 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size,dilation=i,padding='same')\n","        self.conv_2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size,dilation=2*i,padding='same')\n","        self.dp = nn.Dropout(p = dropout_rate)\n","\n","    def forward(self,x):\n","        res_x = self.res_x(x)\n","        x_1 = self.conv_1(x)\n","        x_2 = self.conv_2(x)\n","        x_12 = torch.cat([x_1,x_2])\n","        x = torch.nn.ELU(x_12)\n","        x = self.dp(x)\n","        x = self.conv_3(x)\n","        res_x = x + res_x\n","        return res_x, x\n","\n","class TCN:\n","    def __init__(\n","        self,\n","        num_filters=20,\n","        kernel_size=5,\n","        dilations=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n","        activation='elu',\n","        padding='same',\n","        dropout_rate=0.15\n","    ):\n","        super().__init__()\n","        self.dropout_rate = dropout_rate\n","        self.activation = activation\n","        self.dilations = dilations\n","        self.kernel_size = kernel_size\n","        self.num_filters = num_filters\n","        self.padding = padding\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.fc = nn.Linear(20,2)\n","\n","        if padding != 'causal' and padding != 'same':\n","            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n","        \n","        self.res_blocks = []\n","        for i in range(11):\n","            self.res_blocks.append(ResBlock(2**i, num_filters, self.kernel_size, self.padding, self.dropout_rate))\n","        \n","\n","\n","    def forward(self, inputs):\n","        x = inputs\n","        # gather skip connections, each having a different context\n","        # build the TCN models\n","        for i in range(11):\n","            # feed the output of the previous layer into the next layer\n","            # increase dilation rate for each consecutive layer\n","            x,_ = self.res_blocks[i](x)\n","            # collect skip connection\n","            skip_connections.append(skip_out)\n","        # activate the output of the TCN stack\n","        x = torch.nn.ELU(x)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        # merge the skip connections by simply adding them\n","        return x"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class Hung(nn.Module):\n","    def __init__(self, config, num_inspect_blocks=3):\n","        super().__init__()\n","        self.feature_extractor = HarmonicSTFT(config['sample_rate'])\n","        self.fe_model = ResFrontEnd(6,256,[2,2,2],[2,2,1])\n","        self.spectnt1 = SpecTNTModule(\n","        n_channels=256, n_frequencies=16, n_times=109,\n","        spectral_dmodel=64, spectral_nheads=4, spectral_dimff=64,\n","        temporal_dmodel=256, temporal_nheads=8, temporal_dimff=256,\n","        embed_dim=128, dropout=0.15, use_tct=False, n_block=2)\n","\n","\n","        # self.inspect_module = nn.ModuleList([InSpecTBlock(config) for i in range(num_inspect_blocks)])\n","        self.spectnt2 = SpecTNTModule(\n","        n_channels=256, n_frequencies=16, n_times=109,\n","        spectral_dmodel=64, spectral_nheads=4, spectral_dimff=64,\n","        temporal_dmodel=256, temporal_nheads=8, temporal_dimff=256,\n","        embed_dim=128, dropout=0.15, use_tct=False, n_block=3)\n","        self.fc = nn.Linear(128,3)\n","        self.n_inst = 1\n","    \n","    def forward(self,audio):\n","        spec_embs = []\n","        temp_embs = []\n","        for i in range(self.n_inst):\n","            features = self.feature_extractor(audio[:,:,i])\n","            if len(features.size()) == 3:\n","                features = features.unsqueeze(1)\n","            fe_out = self.fe_model(features)\n","            fe_out = fe_out.permute(0,3,2,1)\n","            spec_emb, temp_emb = self.spectnt1(fe_out)\n","            spec_embs.append(spec_emb)\n","            temp_embs.append(temp_emb)\n","            \n","        spec_embs = torch.stack(spec_embs,dim=4)\n","        temp_embs = torch.stack(temp_embs,dim=4)\n","        # for inspect_block in self.inspect_module:\n","            # spec_embs, temp_embs = inspect_block(spec_embs,temp_embs)\n","        \n","        t_spec_embs = []\n","        t_temp_embs = []\n","        for i in range(self.n_inst):\n","            for block in self.spectnt2.spectnt_blocks:\n","                spec_emb, temp_emb = block(spec_embs[...,i],temp_embs[...,i])\n","            t_spec_embs.append(spec_emb)\n","            t_temp_embs.append(temp_emb)\n","        \n","        spec_embs = torch.stack(t_spec_embs,dim=4)\n","        temp_embs = torch.stack(t_temp_embs,dim=4)\n","        \n","        # spec_emb = None\n","        # temp_emb = None\n","        \n","        temp_embs = torch.mean(temp_embs,dim=4).squeeze()\n","\n","        acts = self.fc(temp_embs)\n","        return acts\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:13.755659Z","iopub.status.busy":"2023-04-27T07:28:13.754576Z","iopub.status.idle":"2023-04-27T07:28:16.980603Z","shell.execute_reply":"2023-04-27T07:28:16.979495Z","shell.execute_reply.started":"2023-04-27T07:28:13.755620Z"},"trusted":true},"outputs":[],"source":["model = InSpecT(config).to(device)\n","# model = Hung(config).to(device)\n","# tcn_model = TCN()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class WeightedCELoss(torch.nn.Module):\n","    def __init__(self):\n","        super(WeightedCELoss, self).__init__()\n","\n","    def forward(self, input, target):\n","        '''\n","            input : B x 3 x F\n","            target : B x 3 x F\n","        '''\n","        \n","        #Computing Class Frequency\n","        class_freqs = torch.sum(target, dim=(0, 2)) / (input.shape[0] * input.shape[2])\n","        weights = 1.0 / class_freqs\n","        #Normalizing Weights\n","        weights /= torch.sum(weights)\n","#         loss = nn.CrossEntropyLoss()(input.permute(0,2,1), target.permute(0,2,1))\n","        loss = nn.CrossEntropyLoss(weight=weights)(input, target)\n","        return loss"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:16.998469Z","iopub.status.busy":"2023-04-27T07:28:16.997497Z","iopub.status.idle":"2023-04-27T07:28:18.627948Z","shell.execute_reply":"2023-04-27T07:28:18.626676Z","shell.execute_reply.started":"2023-04-27T07:28:16.998432Z"},"trusted":true},"outputs":[],"source":["config['learning_rate'] = 1e-3\n","config['loss'] = nn.CrossEntropyLoss()\n","# config['loss'] = WeightedCELoss()\n","config['metrics'] = {\n","#     'MulticlassAccuracy':torchmetrics.classification.MulticlassAccuracy(3,average='none'),\n","    'MultilabelF1Score':torchmetrics.classification.MultilabelF1Score(3,average='none')\n","}\n","# config['optimizer'] = torch.optim.SGD(model.parameters(),lr=config['learning_rate'],momentum=0.9)\n","config['optimizer'] = torch.optim.Adam(model.parameters(),lr=config['learning_rate'])\n","config['epochs'] = 10\n","config['patience'] = 5\n","config['PATH'] = '/raid/home/niranjan20090/DL/models/'\n","config['model_name'] = 'celoss_full_numinst1'"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:18.630326Z","iopub.status.busy":"2023-04-27T07:28:18.629909Z","iopub.status.idle":"2023-04-27T07:28:19.017877Z","shell.execute_reply":"2023-04-27T07:28:19.016639Z","shell.execute_reply.started":"2023-04-27T07:28:18.630286Z"},"trusted":true},"outputs":[],"source":["def test(model,test_loader,config,tcn_model= None):\n","    metric_funs = config['metrics']\n","    sftmax = nn.Softmax(dim=1)\n","    \n","    loss_fun = config['loss']\n","    for metric in metric_funs:\n","        metric_funs[metric].reset()\n","    \n","    model.eval()\n","    running_loss = 0\n","    for metric in metric_funs:\n","        metric_funs[metric].reset()\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(test_loader):\n","            track_splits = batch['audio']\n","            truths = batch['targets'].to(device).permute(0,2,1)\n","            output = []\n","            for i in range(len(track_splits)):\n","                track_splits[i] = track_splits[i].to(device).permute(0,2,1)\n","                output.append(model(track_splits[i]).permute(0,2,1))\n","            output = torch.cat(output,dim=2)\n","                \n","            # tracks = batch['audio'].to(device).permute(0,2,1)\n","\n","            # output = model(tracks).permute(0,2,1)\n","\n","            running_loss += loss_fun(output,truths).detach().cpu().item()\n","\n","            op = output.detach().cpu()\n","            tr = truths.detach().cpu()\n","            \n","            for metric in metric_funs:\n","                metric_funs[metric].update(op,tr)\n","                \n","    \n","    loss = running_loss/len(val_loader)\n","    metrics = {}\n","    for metric in metric_funs:\n","        metrics[metric] = metric_funs[metric].compute()\n","        \n","    return loss,metrics"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:19.022323Z","iopub.status.busy":"2023-04-27T07:28:19.021398Z","iopub.status.idle":"2023-04-27T07:28:19.037087Z","shell.execute_reply":"2023-04-27T07:28:19.036134Z","shell.execute_reply.started":"2023-04-27T07:28:19.022283Z"},"trusted":true},"outputs":[],"source":["def train(model,train_loader,val_loader,config):\n","    running_loss = 0\n","    epochs = config['epochs']\n","    loss_fun = config['loss']\n","    optimizer = config['optimizer']\n","    batch_size = config['batch_size']\n","    metric_funs = config['metrics']\n","    PATH = config['PATH']\n","    \n","    sftmax = nn.Softmax(dim=1)\n","    \n","    best_val_loss = None\n","    best_model = None\n","    best_val_loss_epoch = -1\n","    \n","    for e in range(epochs):\n","        model.train()\n","        running_loss = 0        \n","        \n","        for metric in metric_funs:\n","            metric_funs[metric].reset()\n","        \n","        for batch in tqdm(train_loader,desc=f'Epoch: {e+1}/{epochs}'):\n","            track_splits = batch['audio']\n","            truths = batch['targets'].to(device).permute(0,2,1)\n","            output = []\n","            for i in range(len(track_splits)):\n","                track_splits[i] = track_splits[i].to(device).permute(0,2,1)\n","                output.append(model(track_splits[i]).permute(0,2,1))\n","            output = torch.cat(output,dim=2)\n","\n","            loss = loss_fun(output,truths)\n","            op = output.detach().cpu()\n","            tr = truths.detach().cpu()\n","            \n","            for metric in metric_funs:\n","                metric_funs[metric].update(op,tr)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.detach().cpu().item()\n","        \n","        training_loss = running_loss/len(train_loader)\n","        training_metrics = {}\n","        for metric in metric_funs:\n","            training_metrics[metric] = metric_funs[metric].compute()\n","        \n","        #Validation\n","        validation_loss, validation_metrics = test(model,val_loader,config)\n","        \n","        print(f'Training Loss: {training_loss :.5f} | Validation Loss: {validation_loss :.5f}')\n","        print('Training Metrics: ',end='')\n","        for training_metric in training_metrics:\n","            print(f'{training_metric} : {training_metrics[training_metric]}',end=' ')\n","        print('\\nValidation Metrics: ',end='')\n","        for validation_metric in validation_metrics:\n","            print(f'{validation_metric} : {validation_metrics[validation_metric]}',end=' ')\n","        print()\n","        \n","        if best_val_loss is None or best_val_loss > validation_loss:\n","            print('Model Updated')\n","            best_val_loss = validation_loss\n","            best_val_loss_epoch = e\n","            best_model = model\n","            \n","        print('-'*100)\n","            \n","        if abs(e-best_val_loss_epoch) >= config['patience']:\n","            print('Early Stopping...')\n","            model = best_model\n","            break\n","    \n","    torch.save(best_model.state_dict(), os.path.join(PATH,f'{config[\"model_name\"]}_{best_val_loss :.3f}.pth'))\n","    return best_model"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:28:19.038568Z","iopub.status.busy":"2023-04-27T07:28:19.038298Z","iopub.status.idle":"2023-04-27T07:28:24.917186Z","shell.execute_reply":"2023-04-27T07:28:24.915447Z","shell.execute_reply.started":"2023-04-27T07:28:19.038533Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch: 1/10: 100%|██████████| 347/347 [06:24<00:00,  1.11s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.47776 | Validation Loss: 0.45830\n","Training Metrics: MultilabelF1Score : tensor([0.1183, 0.0120, 0.9300]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1577, 0.0000, 0.9328]) \n","Model Updated\n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 2/10: 100%|██████████| 347/347 [06:29<00:00,  1.12s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.46546 | Validation Loss: 0.47096\n","Training Metrics: MultilabelF1Score : tensor([0.1570, 0.0045, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1649, 0.0036, 0.9328]) \n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 3/10: 100%|██████████| 347/347 [06:38<00:00,  1.15s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.46481 | Validation Loss: 0.46350\n","Training Metrics: MultilabelF1Score : tensor([0.1653, 0.0288, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1707, 0.0249, 0.9328]) \n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 4/10: 100%|██████████| 347/347 [06:43<00:00,  1.16s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.46245 | Validation Loss: 0.45497\n","Training Metrics: MultilabelF1Score : tensor([0.1714, 0.0142, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1686, 0.0000, 0.9328]) \n","Model Updated\n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 5/10: 100%|██████████| 347/347 [06:47<00:00,  1.17s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.45923 | Validation Loss: 0.45248\n","Training Metrics: MultilabelF1Score : tensor([0.1729, 0.0157, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1661, 0.0000, 0.9328]) \n","Model Updated\n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 6/10: 100%|██████████| 347/347 [06:53<00:00,  1.19s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.45807 | Validation Loss: 0.45577\n","Training Metrics: MultilabelF1Score : tensor([0.1731, 0.0064, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1709, 0.0000, 0.9328]) \n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 7/10: 100%|██████████| 347/347 [07:00<00:00,  1.21s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.45829 | Validation Loss: 0.45289\n","Training Metrics: MultilabelF1Score : tensor([0.1731, 0.0159, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1712, 0.0000, 0.9328]) \n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 8/10: 100%|██████████| 347/347 [07:10<00:00,  1.24s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.45767 | Validation Loss: 0.45267\n","Training Metrics: MultilabelF1Score : tensor([1.7315e-01, 2.0233e-04, 9.3182e-01]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1573, 0.0000, 0.9328]) \n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 9/10: 100%|██████████| 347/347 [06:46<00:00,  1.17s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.45772 | Validation Loss: 0.45162\n","Training Metrics: MultilabelF1Score : tensor([0.1707, 0.0010, 0.9318]) \n","Validation Metrics: MultilabelF1Score : tensor([0.1481, 0.0000, 0.9328]) \n","Model Updated\n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 10/10: 100%|██████████| 347/347 [06:39<00:00,  1.15s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 2.07592 | Validation Loss: 0.52421\n","Training Metrics: MultilabelF1Score : tensor([0.1006, 0.0340, 0.9278]) \n","Validation Metrics: MultilabelF1Score : tensor([0.0000, 0.0000, 0.9328]) \n","----------------------------------------------------------------------------------------------------\n"]}],"source":["model = train(model,train_loader,val_loader,config)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model = InSpecT(config).to(device)\n","model.load_state_dict(torch.load(\"/raid/home/niranjan20090/DL/models/celoss_full_numinst1_0.452_final.pth\"), strict=False)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T07:17:28.035232Z","iopub.status.busy":"2023-04-27T07:17:28.034476Z","iopub.status.idle":"2023-04-27T07:17:28.226815Z","shell.execute_reply":"2023-04-27T07:17:28.222972Z","shell.execute_reply.started":"2023-04-27T07:17:28.035192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss:  0.51726\n","Test Metrics: MultilabelF1Score : tensor([0.0000, 0.0000, 0.9328]) Precision : tensor([0.0000, 0.0000, 0.8741]) Recall : tensor([0., 0., 1.]) "]}],"source":["\n","config['metrics'] = {\n","#     'MulticlassAccuracy':torchmetrics.classification.MulticlassAccuracy(3,average='none'),\n","    'MultilabelF1Score':torchmetrics.classification.MultilabelF1Score(3,average='none'),\n","    'Precision':torchmetrics.classification.MultilabelPrecision(3,average='none'),\n","    'Recall':torchmetrics.classification.MultilabelRecall(3,average='none'),\n","}\n","loss, metrics = test(model,test_loader,config)\n","\n","print(f'Test Loss: {loss: .5f}')\n","print('Test Metrics: ',end='')\n","for metric in metrics:{\n","    print(f'{metric} : {metrics[metric]}',end=' ')}"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T05:26:28.412536Z","iopub.status.busy":"2023-04-27T05:26:28.412131Z","iopub.status.idle":"2023-04-27T05:26:28.474161Z","shell.execute_reply":"2023-04-27T05:26:28.473031Z","shell.execute_reply.started":"2023-04-27T05:26:28.412502Z"},"trusted":true},"outputs":[],"source":["#Generating annotations from truths\n","beat_annotations = {k : v.beats.times for k,v in tracks.items() if v.beats is not None}\n","downbeat_annotations = {k : v.beats.times[v.beats.positions == 1] for k,v in tracks.items() if v.beats is not None}"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T05:29:43.868893Z","iopub.status.busy":"2023-04-27T05:29:43.868502Z","iopub.status.idle":"2023-04-27T05:29:43.878017Z","shell.execute_reply":"2023-04-27T05:29:43.876907Z","shell.execute_reply.started":"2023-04-27T05:29:43.868858Z"},"trusted":true},"outputs":[],"source":["#Getting detections over the test dataset.\n","def get_detections(model,loader,beat_tracker,downbeat_tracker):\n","    model.eval()\n","    sftmax = nn.Softmax(dim=1)\n","    detections = {}\n","    \n","    with torch.no_grad():\n","         for batch in loader:\n","            track_splits = batch['audio']\n","            truths = batch['targets'].to(device).permute(0,2,1)\n","            output = []\n","            for i in range(len(track_splits)):\n","                track_splits[i] = track_splits[i].to(device).permute(0,2,1)\n","                output.append(model(track_splits[i]).permute(0,2,1))\n","            output = torch.cat(output,dim=2)\n","            ids = batch['id']\n","\n","            preds = sftmax(output).cpu().detach().numpy()\n","            for i in range(len(ids)):\n","                beats_act = preds[i][0]\n","                beats = beat_tracker(beats_act)\n","                downbeats = downbeat_tracker(preds[i][:2].T)\n","                detections[ids[i]] = {'beats': beats, 'downbeats': downbeats}\n","    return detections"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T05:29:45.698554Z","iopub.status.busy":"2023-04-27T05:29:45.698045Z","iopub.status.idle":"2023-04-27T05:29:47.193482Z","shell.execute_reply":"2023-04-27T05:29:47.192259Z","shell.execute_reply.started":"2023-04-27T05:29:45.698516Z"},"trusted":true},"outputs":[],"source":["from madmom.features.beats import DBNBeatTrackingProcessor\n","from madmom.features.downbeats import DBNDownBeatTrackingProcessor\n","\n","#DBN Beat Tracking Processor from [Bock et. al ISMIR 2016]\n","beat_tracker = DBNBeatTrackingProcessor(\n","    min_bpm=55.0,max_bpm=215.0,fps=config['FPS'],transition_lambda=100,threshold = 0.05\n",")\n","\n","#DBN DownBeat Tracking Processor from [Bock et. al ISMIR 2016]\n","downbeat_tracker = DBNDownBeatTrackingProcessor(\n","    beats_per_bar = [3,4], min_bpm=55.0,max_bpm=215.0,fps=config['FPS'],transition_lambda=100\n",")\n","\n","detections = get_detections(model,test_loader,beat_tracker,downbeat_tracker)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T05:29:58.002466Z","iopub.status.busy":"2023-04-27T05:29:58.001841Z","iopub.status.idle":"2023-04-27T05:29:58.015197Z","shell.execute_reply":"2023-04-27T05:29:58.013793Z","shell.execute_reply.started":"2023-04-27T05:29:58.002415Z"},"trusted":true},"outputs":[],"source":["def evaluate_beats(detections, annotations):\n","    evals = []\n","    for key, det in detections.items():\n","        ann = annotations[key]\n","        e = madmom.evaluation.beats.BeatEvaluation(det, ann)\n","        evals.append(e)\n","    return madmom.evaluation.beats.BeatMeanEvaluation(evals)\n","\n","\n","def evaluate_downbeats(detections, annotations):\n","    evals = []\n","    for key, det in detections.items():\n","        ann = annotations[key]\n","        e = madmom.evaluation.beats.BeatEvaluation(det, ann, downbeats=True)\n","        evals.append(e)\n","    return madmom.evaluation.beats.BeatMeanEvaluation(evals)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-27T05:30:05.341037Z","iopub.status.busy":"2023-04-27T05:30:05.340338Z","iopub.status.idle":"2023-04-27T05:30:05.520816Z","shell.execute_reply":"2023-04-27T05:30:05.518996Z","shell.execute_reply.started":"2023-04-27T05:30:05.341000Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Beat evaluation\n","---------------\n"," Beat tracker: mean for 148 files\n","  F-measure: 0.349 P-score: 0.397 Cemgil: 0.250 Goto: 0.000 CMLc: 0.003 CMLt: 0.014 AMLc: 0.055 AMLt: 0.229 D: 0.229 Dg: 0.001\n"," Downbeat tracker: mean for 148 files\n","  F-measure: 0.349 P-score: 0.397 Cemgil: 0.250 Goto: 0.000 CMLc: 0.003 CMLt: 0.014 AMLc: 0.055 AMLt: 0.229 D: 0.229 Dg: 0.001\n","\n","Downbeat evaluation\n","-------------------\n"," Downbeat tracker: mean for 148 files\n","  F-measure: 0.088 P-score: 0.395 Cemgil: 0.063 Goto: 0.000 CMLc: 0.009 CMLt: 0.020 AMLc: 0.134 AMLt: 0.239 D: 1.062 Dg: 0.005\n"]}],"source":["beat_detections = {k: v['beats'] for k, v in detections.items()}\n","downbeat_detections = {k: v['downbeats'] for k, v in detections.items()}\n","# evaluate beats\n","print('Beat evaluation\\n---------------')\n","print(' Beat tracker:', evaluate_beats(beat_detections, beat_annotations))\n","print(' Downbeat tracker:', evaluate_beats(downbeat_detections, beat_annotations))\n","\n","# evaluate downbeats\n","print('\\nDownbeat evaluation\\n-------------------')\n","print(' Downbeat tracker:', evaluate_downbeats(downbeat_detections, downbeat_annotations))"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["def predict_beat_downbeat(model,audios,postprocessor):\n","    model.eval()\n","    sftmax = nn.Softmax(dim=1)\n","    \n","    with torch.no_grad():\n","        preds = []\n","        for i in range(len(audios)):\n","            wav = torch.Tensor(audios[i]).unsqueeze(0).to(device).permute(0,2,1)\n","            pred = sftmax(model(wav)).cpu().detach().numpy()[:,:2]\n","            preds.append(pred)\n","        preds = np.concatenate(preds)\n","        output = postprocessor(preds)\n","        \n","    return output.squeeze()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions\tAnnotations\n","[[ 0.19  2.    0.04  4.  ]\n"," [ 0.45  3.    0.69  1.  ]\n"," [ 0.7   4.    1.33  2.  ]\n"," [ 0.96  1.    1.97  3.  ]\n"," [ 1.22  2.    2.63  4.  ]\n"," [ 1.47  3.    3.28  1.  ]\n"," [ 1.73  4.    3.93  2.  ]\n"," [ 1.98  1.    4.57  3.  ]\n"," [ 2.24  2.    5.22  4.  ]\n"," [ 2.5   3.    5.87  1.  ]\n"," [ 2.75  4.    6.52  2.  ]\n"," [ 3.01  1.    7.17  3.  ]\n"," [ 3.26  2.    7.82  4.  ]\n"," [ 3.52  3.    8.47  1.  ]\n"," [ 3.78  4.    9.12  2.  ]\n"," [ 4.03  1.    9.77  3.  ]\n"," [ 4.29  2.   10.41  4.  ]\n"," [ 4.54  3.   11.07  1.  ]\n"," [ 4.8   4.   11.71  2.  ]\n"," [ 5.06  1.   12.36  3.  ]]\n"]}],"source":["import IPython\n","idx = 30\n","data = test_dataset[idx]\n","track = gtzan.track(data['id'])\n","audio = np.concatenate(data['audio'],axis=1)\n","num_samples = len(audio[0])\n","beats_per_bar = int(np.amax(track.beats.positions))\n","postprocessor = DBNDownBeatTrackingProcessor(beats_per_bar=beats_per_bar, fps=config['FPS'])\n","\n","output = predict_beat_downbeat(model,data['audio'],postprocessor)\n","bss = np.hstack([track.beats.times.reshape(-1,1),track.beats.positions.reshape(-1,1)])\n","print('Predictions\\tAnnotations')\n","print(np.hstack([output[:20],bss[:20]]).round(2))\n","# output = bss"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# from scipy.io import wavfile\n","# from scipy.signal import resample\n","# sample_rate = config['sample_rate']\n","\n","# beat_sr, beat = wavfile.read('/raid/home/niranjan20090/DL/dataset/beat.wav')\n","# downbeat_sr, downbeat = wavfile.read('/raid/home/niranjan20090/DL/dataset/downbeat.wav')\n","\n","# #Downsample to sample_rate\n","# beat = resample(beat,int(len(beat) * float(sample_rate)/beat_sr))\n","# beat = beat.astype(np.float32)/32768.0\n","# downbeat = resample(downbeat,int(len(downbeat) * float(sample_rate)/downbeat_sr))\n","# downbeat = downbeat.astype(np.float32)/32768.0\n","\n","# beat_length_sample = beat.shape[0]\n","# downbeat_length_sample = downbeat.shape[0]\n","\n","# audio_with_beats = audio[0]\n","# audio_with_beats_an = audio[0]\n","# beat_sequence = np.zeros(num_samples)\n","\n","# for i in range(len(output)):\n","#     beat_start_sample = int(output[i][0]*sample_rate)\n","#     beat_position = output[i][1]\n","#     if beat_position == 1:\n","#         beat_end_sample = beat_start_sample + downbeat_length_sample\n","#         beat_sequence[beat_start_sample:beat_end_sample] = downbeat[:beat_sequence[beat_start_sample:beat_end_sample].shape[0]]\n","#     else:\n","#         beat_end_sample = beat_start_sample + beat_length_sample\n","#         beat_sequence[beat_start_sample:beat_end_sample] = beat[:beat_sequence[beat_start_sample:beat_end_sample].shape[0]]\n","\n","# audio_scale_factor = (np.amax(beat_sequence) - np.amin(beat_sequence))/(np.amax(audio_with_beats)-np.amin(audio_with_beats))        \n","# audio_with_beats = audio_scale_factor*0.4*audio_with_beats + 0.5*beat_sequence\n","# beat_sequence = np.zeros(num_samples)\n","\n","# for i in range(len(bss)):\n","#     beat_start_sample = int(bss[i][0]*sample_rate)\n","#     beat_position = bss[i][1]\n","#     if beat_position == 1:\n","#         beat_end_sample = beat_start_sample + downbeat_length_sample\n","#         beat_sequence[beat_start_sample:beat_end_sample] = downbeat[:beat_sequence[beat_start_sample:beat_end_sample].shape[0]]\n","#     else:\n","#         beat_end_sample = beat_start_sample + beat_length_sample\n","#         beat_sequence[beat_start_sample:beat_end_sample] = beat[:beat_sequence[beat_start_sample:beat_end_sample].shape[0]]\n","\n","# audio_scale_factor = (np.amax(beat_sequence) - np.amin(beat_sequence))/(np.amax(audio_with_beats_an)-np.amin(audio_with_beats_an))        \n","# audio_with_beats_an = audio_scale_factor*0.4*audio_with_beats_an + 0.5*beat_sequence\n","\n","# print('Audio')\n","# IPython.display.display(IPython.display.Audio(audio,rate=sample_rate))\n","# print('Audio with beats and downbeats (annotated)')\n","# IPython.display.display(IPython.display.Audio(audio_with_beats_an,rate=sample_rate))\n","# print('Audio with beats and downbeats (predicted)')\n","# IPython.display.display(IPython.display.Audio(audio_with_beats,rate=sample_rate))"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# # song = test_dataset.track_ids[idx]\n","# import matplotlib.pyplot as plt\n","# det = detections[data['id']]\n","# # print(det)\n","\n","# # track = tracks[song]\n","# # audio\n","# hop_length = 512\n","\n","# spec = librosa.amplitude_to_db(np.abs(librosa.stft(audio[0], hop_length=hop_length)), ref=np.max)\n","# # librosa.display.specshow(spec, y_axis='log', sr=sr, hop_length=hop_length, x_axis='time')\n","# plt.plot(spec)\n","# plt.title(f'Log-frequency power spectrogram of track: \"{data[\"id\"]}\"')\n","# # plt.colorbar(format=\"%+2.f dB\")\n","# # plot annotations in the upper part\n","# plt.vlines(track.beats.times, hop_length * 2, sample_rate / 2, linestyles='dotted', color='w')\n","# plt.vlines(track.beats.times[track.beats.positions == 1], hop_length * 2, sr / 2, color='w')\n","# plt.text(7, hop_length * 1.65, 'Annotations (above)', color='w', fontsize=12)\n","# # plot detections in the lower part\n","# plt.vlines(det['downbeats'][:, 0], 0, hop_length, linestyles='dotted', color='w')\n","# plt.vlines(det['downbeats'][det['downbeats'][:, 1] == 1][:, 0], 0, hop_length, color='w')\n","# plt.text(7, hop_length * 1.1, 'Detections (below)', color='w', fontsize=12)\n","# plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
